# Chat completions

> Creates a model response for the given chat conversation.

## OpenAPI

````yaml post /chat/completions
paths:
  path: /chat/completions
  method: post
  servers:
    - url: https://api.siliconflow.com/v1
  request:
    security:
      - title: bearerAuth
        parameters:
          query: {}
          header:
            Authorization:
              type: http
              scheme: bearer
              description: >-
                Use the following format for authentication: Bearer [<your api
                key>](https://cloud.siliconflow.com/account/ak)
          cookie: {}
    parameters:
      path: {}
      query: {}
      header: {}
      cookie: {}
    body:
      application/json:
        schemaArray:
          - type: object
            properties:
              model:
                allOf:
                  - type: string
                    description: >-
                      Corresponding Model Name. To better enhance service
                      quality, we will make periodic changes to the models
                      provided by this service, including but not limited to
                      model on/offlining and adjustments to model service
                      capabilities. We will notify you of such changes through
                      appropriate means such as announcements or message pushes
                      where feasible.
                    example: deepseek-ai/DeepSeek-V3
                    default: deepseek-ai/DeepSeek-V3
                    enum:
                      - inclusionAI/Ling-mini-2.0
                      - ByteDance-Seed/Seed-OSS-36B-Instruct
                      - Qwen/Qwen3-30B-A3B-Instruct-2507
                      - Qwen/Qwen3-235B-A22B-Thinking-2507
                      - Qwen/Qwen3-235B-A22B-Instruct-2507
                      - baidu/ERNIE-4.5-300B-A47B
                      - moonshotai/Kimi-K2-Instruct-0905
                      - moonshotai/Kimi-K2-Instruct
                      - ascend-tribe/pangu-pro-moe
                      - tencent/Hunyuan-A13B-Instruct
                      - MiniMaxAI/MiniMax-M1-80k
                      - Qwen/QwQ-32B
                      - Qwen/Qwen2.5-14B-Instruct
                      - Qwen/Qwen2.5-32B-Instruct
                      - Qwen/Qwen2.5-72B-Instruct
                      - Qwen/Qwen2.5-72B-Instruct-128K
                      - Qwen/Qwen2.5-7B-Instruct
                      - Qwen/Qwen2.5-Coder-32B-Instruct
                      - Qwen/Qwen2.5-VL-32B-Instruct
                      - Qwen/Qwen2.5-VL-72B-Instruct
                      - Qwen/Qwen2.5-VL-7B-Instruct
                      - Qwen/Qwen3-14B
                      - Qwen/Qwen3-235B-A22B
                      - Qwen/Qwen3-235B-A22B-Instruct-2507
                      - Qwen/Qwen3-235B-A22B-Thinking-2507
                      - Qwen/Qwen3-30B-A3B
                      - Qwen/Qwen3-30B-A3B-Instruct-2507
                      - Qwen/Qwen3-30B-A3B-Thinking-2507
                      - Qwen/Qwen3-32B
                      - Qwen/Qwen3-8B
                      - Qwen/Qwen3-Coder-30B-A3B-Instruct
                      - Qwen/Qwen3-Coder-480B-A35B-Instruct
                      - THUDM/GLM-4-32B-0414
                      - THUDM/GLM-4-9B-0414
                      - THUDM/GLM-4.1V-9B-Thinking
                      - THUDM/GLM-Z1-32B-0414
                      - THUDM/GLM-Z1-9B-0414
                      - baidu/ERNIE-4.5-300B-A47B
                      - deepseek-ai/DeepSeek-R1
                      - deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
                      - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
                      - deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
                      - deepseek-ai/DeepSeek-V3.1
                      - deepseek-ai/DeepSeek-V3
                      - deepseek-ai/deepseek-vl2
                      - meta-llama/Meta-Llama-3.1-8B-Instruct
                      - stepfun-ai/step3
                      - tencent/Hunyuan-A13B-Instruct
                      - zai-org/GLM-4.5
                      - zai-org/GLM-4.5-Air
                      - zai-org/GLM-4.5V
              messages:
                allOf:
                  - type: array
                    description: A list of messages comprising the conversation so far.
                    items:
                      type: object
                      properties:
                        role:
                          type: string
                          description: >-
                            The role of the messages author. Choice between:
                            system, user, or assistant.
                          example: user
                          default: user
                          enum:
                            - user
                            - assistant
                            - system
                        content:
                          oneOf:
                            - type: string
                              description: The contents of the message.
                              example: >-
                                What opportunities and challenges will the
                                Chinese large model industry face in 2025?
                              default: >-
                                What opportunities and challenges will the
                                Chinese large model industry face in 2025?
                      required:
                        - role
                        - content
                    minItems: 1
                    maxItems: 10
              stream:
                allOf:
                  - type: boolean
                    description: >-
                      If set, tokens are returned as Server-Sent Events as they
                      are made available. Stream terminates with `data: [DONE]`
                    example: false
              max_tokens:
                allOf:
                  - type: integer
                    description: >
                      The maximum number of tokens to generate. The max_tokens
                      is equal to the context length. Since some model inference
                      services are still being updated, please do not set
                      max_tokens to the maximum value (context length) when
                      making a request. It is recommended to reserve around 10k
                      as space for input content
                    example: 4096
                    minimum: 1
                    maximum: 16384
              enable_thinking:
                allOf:
                  - type: boolean
                    description: >
                      Switches between thinking and non-thinking modes. Default
                      is True.  This field supports the following models: 

                          - Qwen/Qwen3-8B
                          - Qwen/Qwen3-14B
                          - Qwen/Qwen3-32B
                          - wen/Qwen3-30B-A3B
                          - Qwen/Qwen3-235B-A22B
                          - tencent/Hunyuan-A13B-Instruct
                          - zai-org/GLM-4.5V
                          - deepseek-ai/DeepSeek-V3.1

                      If you want to use the function call feature for
                      deepseek-ai/DeepSeek-V3.1, you need to set enable_thinking
                      to false. 
                    example: false
              thinking_budget:
                allOf:
                  - type: integer
                    description: >-
                      Maximum number of tokens for chain-of-thought output. This
                      field applies to all Reasoning models.
                    example: 4096
                    default: 4096
                    minimum: 128
                    maximum: 32768
              min_p:
                allOf:
                  - type: number
                    description: >-
                      Dynamic filtering threshold that adapts based on token
                      probabilities.This field only applies to Qwen3.
                    format: float
                    example: 0.05
                    minimum: 0
                    maximum: 1
              stop:
                allOf:
                  - description: >
                      Up to 4 sequences where the API will stop generating
                      further tokens. The returned text will not contain the
                      stop sequence.
                    nullable: true
                    oneOf:
                      - type: string
                        example: null
                        nullable: true
                      - type: array
                        minItems: 1
                        maxItems: 4
                        items:
                          type: string
                          example: 'null'
              temperature:
                allOf:
                  - type: number
                    description: Determines the degree of randomness in the response.
                    format: float
                    example: 0.7
              top_p:
                allOf:
                  - type: number
                    description: >-
                      The `top_p` (nucleus) parameter is used to dynamically
                      adjust the number of choices for each predicted token
                      based on the cumulative probabilities.
                    format: float
                    example: 0.7
                    default: 0.7
              top_k:
                allOf:
                  - type: number
                    format: float
                    example: 50
              frequency_penalty:
                allOf:
                  - type: number
                    format: float
                    example: 0.5
              'n':
                allOf:
                  - type: integer
                    description: Number of generations to return
                    example: 1
              response_format:
                allOf:
                  - type: object
                    description: >-
                      An object specifying the format that the model must
                      output.
                    properties:
                      type:
                        type: string
                        description: The type of the response format.
                        example: text
              tools:
                allOf:
                  - type: array
                    description: >
                      A list of tools the model may call. Currently, only
                      functions are supported as a tool. Use this to provide a
                      list of functions the model may generate JSON inputs for.
                      A max of 128 functions are supported.
                    items:
                      $ref: '#/components/schemas/ChatCompletionTool'
            title: LLM
            refIdentifier: '#/components/schemas/ChatCompletionRequest'
            requiredProperties:
              - model
              - messages
          - type: object
            properties:
              model:
                allOf:
                  - type: string
                    description: >-
                      Corresponding Model Name. To better enhance service
                      quality, we will make periodic changes to the models
                      provided by this service, including but not limited to
                      model on/offlining and adjustments to model service
                      capabilities. We will notify you of such changes through
                      appropriate means such as announcements or message pushes
                      where feasible.
                    example: Qwen2.5-VL-32B-Instruct
                    default: Qwen2.5-VL-32B-Instruct
                    enum:
                      - THUDM/GLM-4.1V-9B-Thinking
                      - Qwen/Qwen2.5-VL-32B-Instruct
                      - Qwen/Qwen2.5-VL-72B-Instruct
                      - deepseek-ai/deepseek-vl2
              messages:
                allOf:
                  - type: array
                    description: A list of messages comprising the conversation so far.
                    items:
                      type: object
                      properties:
                        role:
                          type: string
                          description: >-
                            The role of the messages author. Choice between:
                            system, user, or assistant.
                          example: user
                          default: user
                          enum:
                            - user
                            - assistant
                            - system
                        content:
                          oneOf:
                            - type: array
                              description: >-
                                An array of content parts with a defined type,
                                each can be of type `text` or `image_url` when
                                passing in images. You can pass multiple images
                                by adding multiple `image_url` content parts.
                              items:
                                $ref: >-
                                  #/components/schemas/ChatCompletionRequestUserMessageContentPart
                              minItems: 1
                      required:
                        - role
                        - content
                    minItems: 1
                    maxItems: 10
              stream:
                allOf:
                  - type: boolean
                    description: >-
                      If set, tokens are returned as Server-Sent Events as they
                      are made available. Stream terminates with `data: [DONE]`
                    example: false
                    default: false
              max_tokens:
                allOf:
                  - type: integer
                    description: The maximum number of tokens to generate.
                    example: 512
                    default: 512
                    minimum: 1
                    maximum: 4096
              stop:
                allOf:
                  - description: >
                      Up to 4 sequences where the API will stop generating
                      further tokens. The returned text will not contain the
                      stop sequence.
                    default: []
                    nullable: true
                    oneOf:
                      - type: array
                        minItems: 1
                        maxItems: 4
                        items:
                          type: string
                          example: 'null'
                      - type: string
                        default: <|endoftext|>
                        example: |+

                        nullable: true
                      - type: string
                        default: <|endoftext|>
                        example: ''
                        nullable: true
              temperature:
                allOf:
                  - type: number
                    description: Determines the degree of randomness in the response.
                    format: float
                    example: 0.7
                    default: 0.7
              top_p:
                allOf:
                  - type: number
                    description: >-
                      The `top_p` (nucleus) parameter is used to dynamically
                      adjust the number of choices for each predicted token
                      based on the cumulative probabilities.
                    format: float
                    example: 0.7
                    default: 0.7
              top_k:
                allOf:
                  - type: number
                    format: float
                    example: 50
                    default: 50
              frequency_penalty:
                allOf:
                  - type: number
                    format: float
                    example: 0.5
                    default: 0.5
              'n':
                allOf:
                  - type: integer
                    description: Number of generations to return
                    example: 1
                    default: 1
              response_format:
                allOf:
                  - type: object
                    description: >-
                      An object specifying the format that the model must
                      output.
                    properties:
                      type:
                        type: string
                        description: The type of the response format.
                        example: text
            title: VLM
            refIdentifier: '#/components/schemas/ChatCompletionVLMRequest'
            requiredProperties:
              - model
              - messages
        examples:
          example:
            value:
              model: deepseek-ai/DeepSeek-V3
              messages:
                - role: user
                  content: >-
                    What opportunities and challenges will the Chinese large
                    model industry face in 2025?
              stream: false
              max_tokens: 4096
              enable_thinking: false
              thinking_budget: 4096
              min_p: 0.05
              stop: null
              temperature: 0.7
              top_p: 0.7
              top_k: 50
              frequency_penalty: 0.5
              'n': 1
              response_format:
                type: text
              tools:
                - type: function
                  function:
                    description: <string>
                    name: <string>
                    parameters: {}
                    strict: false
  response:
    '200':
      application/json:
        schemaArray:
          - type: object
            properties:
              id:
                allOf:
                  - type: string
              choices:
                allOf:
                  - $ref: '#/components/schemas/ChatCompletionChoicesData'
              usage:
                allOf:
                  - $ref: '#/components/schemas/UsageData'
              created:
                allOf:
                  - type: integer
              model:
                allOf:
                  - type: string
              object:
                allOf:
                  - type: string
                    enum:
                      - chat.completion
            refIdentifier: '#/components/schemas/ChatCompletionResponse'
        examples:
          example:
            value:
              id: <string>
              choices:
                - message:
                    role: assistant
                    content: <string>
                    reasoning_content: <string>
                    tool_calls:
                      - id: <string>
                        type: function
                        function:
                          name: <string>
                          arguments: <string>
                  finish_reason: stop
              usage:
                prompt_tokens: 123
                completion_tokens: 123
                total_tokens: 123
              created: 123
              model: <string>
              object: chat.completion
        description: '200'
      text/event-stream:
        schemaArray:
          - type: object
            properties:
              id:
                allOf:
                  - type: string
              choices:
                allOf:
                  - $ref: '#/components/schemas/ChatCompletionChoicesData'
              created:
                allOf:
                  - type: integer
              model:
                allOf:
                  - type: string
              object:
                allOf:
                  - type: string
                    enum:
                      - chat.completion.chunk
            refIdentifier: '#/components/schemas/ChatCompletionStream'
        examples:
          example:
            value:
              id: <string>
              choices:
                - message:
                    role: assistant
                    content: <string>
                    reasoning_content: <string>
                    tool_calls:
                      - id: <string>
                        type: function
                        function:
                          name: <string>
                          arguments: <string>
                  finish_reason: stop
              created: 123
              model: <string>
              object: chat.completion.chunk
        description: '200'
    '400':
      application/json:
        schemaArray:
          - type: object
            properties:
              code:
                allOf:
                  - type: integer
                    nullable: true
                    default: false
                    example: 20012
              message:
                allOf:
                  - type: string
                    nullable: false
              data:
                allOf:
                  - type: string
                    nullable: false
            refIdentifier: '#/components/schemas/BadRquestData'
            requiredProperties:
              - message
              - data
              - code
        examples:
          example:
            value:
              code: 20012
              message: <string>
              data: <string>
        description: BadRequest
    '401':
      application/json:
        schemaArray:
          - type: string
            refIdentifier: '#/components/schemas/UnauthorizedData'
            default: false
            example: Invalid token
        examples:
          example:
            value: Invalid token
        description: Unauthorized
    '404':
      application/json:
        schemaArray:
          - type: string
            refIdentifier: '#/components/schemas/NotFoundData'
            default: false
            example: 404 page not found
        examples:
          example:
            value: 404 page not found
        description: NotFound
    '429':
      application/json:
        schemaArray:
          - type: object
            properties:
              message:
                allOf:
                  - type: string
                    example: >-
                      Request was rejected due to rate limiting. If you want
                      more, please contact contact@siliconflow.com. Details:TPM
                      limit reached.
              data:
                allOf:
                  - type: string
            refIdentifier: '#/components/schemas/RateLimitData'
            requiredProperties:
              - message
              - data
        examples:
          example:
            value:
              message: >-
                Request was rejected due to rate limiting. If you want more,
                please contact contact@siliconflow.com. Details:TPM limit
                reached.
              data: <string>
        description: RateLimit
    '503':
      application/json:
        schemaArray:
          - type: object
            properties:
              code:
                allOf:
                  - type: integer
                    example: 50505
              message:
                allOf:
                  - type: string
                    example: Model service overloaded. Please try again later.
              data:
                allOf:
                  - type: string
                    nullable: false
            refIdentifier: '#/components/schemas/OverloadedtData'
            requiredProperties:
              - code
              - message
              - data
        examples:
          example:
            value:
              code: 50505
              message: Model service overloaded. Please try again later.
              data: <string>
        description: Overloaded
    '504':
      application/json:
        schemaArray:
          - type: string
            refIdentifier: '#/components/schemas/TimeoutData'
        examples:
          example:
            value: <string>
        description: Timeout
  deprecated: false
  type: path
components:
  schemas:
    FinishReason:
      type: string
      enum:
        - stop
        - eos
        - length
        - tool_calls
    UsageData:
      type: object
      properties:
        prompt_tokens:
          type: integer
        completion_tokens:
          type: integer
        total_tokens:
          type: integer
    ChatCompletionChoicesData:
      type: array
      items:
        type: object
        properties:
          message:
            type: object
            properties:
              role:
                type: string
                example: assistant
              content:
                type: string
              reasoning_content:
                description: >-
                  Only the deepseek-R1 series and Qwen/QwQ-32B models support
                  reasoning_content. This part returns the reasoning content,
                  which is at the same level as the content. In each round of
                  the conversation, the model outputs the reasoning chain
                  content (reasoning_content) and the final answer (content). In
                  the next round of the conversation, the reasoning chain
                  content from previous rounds will not be appended to the
                  context.
                type: string
              tool_calls:
                type: array
                description: The tool calls generated by the model, such as function calls.
                items:
                  $ref: '#/components/schemas/ChatCompletionMessageToolCall'
          finish_reason:
            $ref: '#/components/schemas/FinishReason'
    ChatCompletionRequestMessageContentPartText:
      type: object
      title: Text content part
      properties:
        type:
          type: string
          enum:
            - text
          description: The type of the content part.
          default: text
        text:
          type: string
          description: The text content.
          default: Describe this picture.
      required:
        - type
        - text
    ChatCompletionRequestMessageContentPartImage:
      type: object
      title: Image content part
      properties:
        type:
          type: string
          enum:
            - image_url
          description: The type of the content part.
          default: image_url
        image_url:
          type: object
          properties:
            url:
              type: string
              description: >-
                Either a URL of the image or the base64 encoded image data.
                TeleAI/TeleMM only support the base64 encoded image data.
              default: >-
                https://sf-maas.s3.us-east-1.amazonaws.com/images/recu6XreBFQ0st.png
              example: >-
                https://sf-maas.s3.us-east-1.amazonaws.com/images/recu6XreBFQ0st.png
            detail:
              type: string
              description: Specifies the detail level of the image.
              enum:
                - auto
                - low
                - high
              default: auto
          required:
            - url
      required:
        - type
        - image_url
    ChatCompletionRequestUserMessageContentPart:
      oneOf:
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartImage'
        - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      x-oaiExpandable: true
    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that the model does not always
                generate valid JSON, and may hallucinate parameters not defined
                by your function schema. Validate the arguments in your code
                before calling your function.
          required:
            - name
            - arguments
      required:
        - id
        - type
        - function
    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: '#/components/schemas/FunctionObject'
      required:
        - type
        - function
    FunctionObject:
      type: object
      properties:
        description:
          type: string
          description: >-
            A description of what the function does, used by the model to choose
            when and how to call the function.
        name:
          type: string
          description: >-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or
            contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
        strict:
          type: boolean
          nullable: true
          default: false
          description: >-
            Whether to enable strict schema adherence when generating the
            function call. If set to true, the model will follow the exact
            schema defined in the `parameters` field. Only a subset of JSON
            Schema is supported when `strict` is `true`. Learn more about
            Structured Outputs in the [function calling
            guide](docs/guides/function-calling).
      required:
        - name
    FunctionParameters:
      type: object
      description: >-
        The parameters the functions accepts, described as a JSON Schema object.
        See the [guide](/guides/function_calling) for examples, and the [JSON
        Schema reference](https://json-schema.org/understanding-json-schema/)
        for documentation about the format. 


        Omitting `parameters` defines a function with an empty parameter list.
      additionalProperties: true

````