Edit my AI server code so it uses OpenRouter to call DeepSeek models (instead of calling the DeepSeek endpoint directly). Make all changes in the existing ai-server/server.js (or server.js if that's where my DeepSeek call lives). Do NOT hardcode any API keys — use environment variable OPENROUTER_KEY.

Requirements — code changes:
1. Replace the existing HTTP call to DeepSeek with a request to:
   URL: https://openrouter.ai/api/v1/chat/completions
   Headers:
     Authorization: Bearer ${process.env.OPENROUTER_KEY}
     Content-Type: application/json
2. Send the model name in the request body. Use this prioritized list and try each in order (stop on the first that returns a valid response):
   - "openrouter/deepseek/deepseek-chat-v3.1:free"
   - "openrouter/deepseek/deepseek-v3.1"
   - "openrouter/deepseek/deepseek-coder-v2-instruct"
   - "openrouter/deepseek/deepseek-coder-v2"
   If the chosen model returns error "model not found" or HTTP 404 from OpenRouter, automatically try the next model in the list.
3. Keep the same messages structure as before (system/user roles). Use temperature 0.12 and a reasonable max_tokens (4000). Make the request timeout configurable (env var OPENROUTER_TIMEOUT, default 120000 ms).
4. Add robust error handling and clear logging. If OpenRouter returns an error, return JSON to the client with {ok:false, provider:'openrouter', detail: <message>} so the UI can show it.
5. Add a new convenient route /api/test-openrouter that:
   - accepts GET and returns a small test chat completion using the first available model above and prompt: "Say 'openrouter test ok'".
   - returns {ok:true, model: <modelUsed>, response: <assistantText>} on success or {ok:false, error: <...>} on failure.
6. Ensure the response parsing extracts the assistant text correctly (OpenRouter responses follow OpenAI ChatCompletion format: resp.choices[0].message.content or resp.choices[0].text). Handle both variants and return the assistant text to callers.
7. Keep existing JSON-parsing logic that expects the model to return JSON when you ask it for structured files. Do not change the JSON-extraction/parsing behavior — just swap transport to OpenRouter.
8. Add friendly console logs that instruct the developer how to set the secret in Replit, for example:
   - console.log("Set OPENROUTER_KEY in Replit secrets and restart the ai server");

Deliverables:
- Show a minimal patch of changed lines (the new fetch/axios call block) and the full new /api/test-openrouter endpoint code.
- In the commit message include: "switch to OpenRouter (deepseek) + test endpoint".
- At the end, add a short usage note for me (plain text) telling me how to set the Replit secret OPENROUTER_KEY, how to run the ai server (node ai-server/server.js or npm run ai) and which URL to open to hit /api/test-openrouter.

Important safety & behavior:
- Do NOT print any API keys.
- Do NOT enable auto-apply of code changes — keep AUTO_APPLY behavior unchanged.
- Keep the same content-type and message flow so my frontend/chat UI continues to work unchanged.

If anything about the repo structure is unclear (file is named server.js vs ai-server/server.js), edit both possible files (prefer ai-server/server.js) or update both; show what you changed.